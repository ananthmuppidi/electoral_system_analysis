{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dython.nominal import associations\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Preparation\n",
    "1. Data is filtered to only include 2014 and higher data\n",
    "2. Columns are split into categorical and numerical columns logically, for further proessing and training\n",
    "3. Since we are trying to predict the deposit_lost, we remove vote shares and margin shares since having these would make the problem redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../assets/data/cleaned_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df[df[\"Year\"] > 2014]\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Sex', 'Party', 'Candidate_Type', 'Constituency_Type', 'Party_Type_TCPD', 'Same_Constituency','Same_Party',\n",
    "                       'Turncoat', 'Incumbent', 'Recontest', 'MyNeta_education',\n",
    "                       'TCPD_Prof_Main', 'Deposit_Lost', 'last_poll']\n",
    "\n",
    "numerical_columns = ['Electors', 'N_Cand', 'Turnout_Percentage',\n",
    "                     'Vote_Share_Percentage', 'Margin_Percentage', 'ENOP','No_Terms', 'Contested']\n",
    "df = df[categorical_columns + numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the voting data since that would make the classifier redundant\n",
    "df = df.drop(columns=['Vote_Share_Percentage', 'Margin_Percentage'])\n",
    "numerical_columns = ['Electors', 'N_Cand', 'Turnout_Percentage', 'ENOP','No_Terms', 'Contested']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Data Preprocessing\n",
    "1. The categorical columns are numerically encoded so that they can further be passed into a classifier\n",
    "2. The numerical columns are normalized so that differences in absolute numbers wont affect the classification problem.\n",
    "3. Traning, Testing data are prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[categorical_columns] = df[categorical_columns].apply(label_encoder.fit_transform)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "X = df.drop(columns=['Deposit_Lost'])\n",
    "y = df['Deposit_Lost']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Various classification algorithms were chosen from the scikit learn library\n",
    "2. Models were trained using all these algorithms, and the necessary metrics are printed for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| Model               |   Accuracy |   Precision |   Recall |   F1 Score |   CV Accuracy | Confusion Matrix   |\n",
      "+=====================+============+=============+==========+============+===============+====================+\n",
      "| Random Forest       |   0.967242 |    0.979972 | 0.982007 |   0.980989 |      0.96202  | TN: 205  FP: 29    |\n",
      "|                     |            |             |          |            |               | FN: 26  TP: 1419   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| Gradient Boosting   |   0.96486  |    0.980583 | 0.978547 |   0.979564 |      0.96202  | TN: 206  FP: 28    |\n",
      "|                     |            |             |          |            |               | FN: 31  TP: 1414   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| SVM                 |   0.860631 |    0.860631 | 1        |   0.925096 |      0.856122 | TN: 0  FP: 234     |\n",
      "|                     |            |             |          |            |               | FN: 0  TP: 1445    |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| Logistic Regression |   0.897558 |    0.911441 | 0.975779 |   0.942513 |      0.884421 | TN: 97  FP: 137    |\n",
      "|                     |            |             |          |            |               | FN: 35  TP: 1410   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| K-Nearest Neighbors |   0.927933 |    0.952804 | 0.964014 |   0.958376 |      0.922401 | TN: 165  FP: 69    |\n",
      "|                     |            |             |          |            |               | FN: 52  TP: 1393   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| Naive Bayes         |   0.895176 |    0.934887 | 0.943945 |   0.939394 |      0.889783 | TN: 139  FP: 95    |\n",
      "|                     |            |             |          |            |               | FN: 81  TP: 1364   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n",
      "| Decision Tree       |   0.957117 |    0.971821 | 0.978547 |   0.975172 |      0.951296 | TN: 193  FP: 41    |\n",
      "|                     |            |             |          |            |               | FN: 31  TP: 1414   |\n",
      "+---------------------+------------+-------------+----------+------------+---------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('SVM', SVC(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "    cv_accuracy = accuracy_score(y_train, cv_scores)\n",
    "    \n",
    "    confusion_str = f\"TN: {confusion_mat[0][0]}  FP: {confusion_mat[0][1]}\\n\" \\\n",
    "                    f\"FN: {confusion_mat[1][0]}  TP: {confusion_mat[1][1]}\"\n",
    "    \n",
    "    results.append([model_name, accuracy, precision, recall, f1, cv_accuracy, confusion_str])\n",
    "\n",
    "headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"CV Accuracy\", \"Confusion Matrix\"]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: The ratio of correctly predicted instances to the total instances in a dataset.\n",
    "- Precision: The ratio of true positive predictions to the total positive predictions (true positive + false positive): Useful when false positives are crucial to avoid\n",
    "- Recall: The ratio of true positive predictions to the total actual positive instances (true positive + false negative): Usefdul when false negatives are crucial to avoid\n",
    "- F1 Score: The harmonic mean of precision and recall, providing a balanced measure of both metrics. It considers false positives and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Based on the above code, we can notice that the Random Forest and and fradient boosting classifier had the highest accuracy and CV accuracy, while also having confusion matrices with the main daigonal being heavy. Hence, these can be used for an optimal classification.\n",
    "\n",
    "### Inferences: \n",
    "- This classifier can be used to predict, without having the current voting data, wether a candidate will have thier deposit lost. \n",
    "- The RandomForest classifier is the most accurate out of different popular models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------------+----------+------------+---------------+--------------------+----------------------------------+\n",
      "| Model             |   Accuracy |   Precision |   Recall |   F1 Score |   CV Accuracy | Confusion Matrix   | Top 3 Features                   |\n",
      "+===================+============+=============+==========+============+===============+====================+==================================+\n",
      "| Random Forest     |   0.967242 |    0.979972 | 0.982007 |   0.980989 |       0.96202 | TN: 205  FP: 29    | No_Terms, Party_Type_TCPD, Party |\n",
      "|                   |            |             |          |            |               | FN: 26  TP: 1419   |                                  |\n",
      "+-------------------+------------+-------------+----------+------------+---------------+--------------------+----------------------------------+\n",
      "| Gradient Boosting |   0.96486  |    0.980583 | 0.978547 |   0.979564 |       0.96202 | TN: 206  FP: 28    | No_Terms, Party_Type_TCPD, Party |\n",
      "|                   |            |             |          |            |               | FN: 31  TP: 1414   |                                  |\n",
      "+-------------------+------------+-------------+----------+------------+---------------+--------------------+----------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "    top_features_indices = importances.argsort()[-3:][::-1]\n",
    "    top_features = [X_train.columns[i] for i in top_features_indices]\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_predict(model, X_train, y_train, cv=kfold)\n",
    "    cv_accuracy = accuracy_score(y_train, cv_scores)\n",
    "    \n",
    "    confusion_str = f\"TN: {confusion_mat[0][0]}  FP: {confusion_mat[0][1]}\\n\" \\\n",
    "                    f\"FN: {confusion_mat[1][0]}  TP: {confusion_mat[1][1]}\"\n",
    "    \n",
    "    results.append([model_name, accuracy, precision, recall, f1, cv_accuracy, confusion_str, ', '.join(top_features)])\n",
    "    \n",
    "headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"CV Accuracy\", \"Confusion Matrix\", \"Top 3 Features\"]\n",
    "print(tabulate(results, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Based on the above code, we can see that for both the chosen classifiers above, The top features for prediction are the same.\n",
    "### Inferences: \n",
    "- No_Terms, which tells us the number of terms won by a candidate in the past, along Party_Type_TCPD and Party are the main indicators of wether a candidate will lose thier deposit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Inferences: \n",
    "\n",
    "- This classification problem can be used to predict, without having the current voting data, wether a candidate will have thier deposit lost. \n",
    "- The RandomForest classifier is the most accurate out of different popular models.\n",
    "- No_Terms, which tells us the number of terms won by a candidate in the past, along Party_Type_TCPD and Party are the main indicators of wether a candidate will lose thier deposit.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
